{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3859d",
   "metadata": {},
   "source": [
    "# Handwritten DIGITalization \n",
    "\n",
    "We used TensorFlow, an open-source software library and ‘MNIST’ data set adapted from\n",
    "Homepage: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "| Split | Example |\n",
    "|-|-|\n",
    "|test|10,000|\n",
    "|train|60,000|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0908ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "handDigitalizationDataset = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165616e7",
   "metadata": {},
   "source": [
    "The `handDigitalizationDataset` contains data of handwritten characters based on a 28x28 pixels images of 0 to 9 as shown below\n",
    "![28x28 pixels images of 0 to 9](./img/mnist-3.0.1.png \"28x28 pixels images of 0 to 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b8006",
   "metadata": {},
   "source": [
    "# Spliting the Data\n",
    "`handDigitalizationDataset` will be devided to two groups of `test` and `train` data.\n",
    "\n",
    "As we are dealing with a two diementional data (rows and columns) so we have to create two sets of test and train data for each diemention.\n",
    "We know that each row contains image data and each column contains the label (value).\n",
    "for train data we have created `image_train` and `label_train` and for test data `image_test` and `label_test`.\n",
    "\n",
    "**_NOTE:_** Testing should never be part of training data since it will assist us to validate the performance of our neural network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7357f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(image_train, label_train), (image_test, label_test) = handDigitalizationDataset.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c93a7",
   "metadata": {},
   "source": [
    "To verify the data dimension, we may use the `shape` command, which is a tuple that always provides the array's dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92020e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad84e9e",
   "metadata": {},
   "source": [
    "# Loading Image Library\n",
    "To plot the images in the dataset we rely on `matplotlib.pyplot` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591e7e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "plot.imshow(image_train[0])\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a6fb5",
   "metadata": {},
   "source": [
    "## The image matrix\n",
    "We can see that the image data in the dataset are consist of colored image \n",
    "\n",
    "![colored one](./img/one-color.png \"colored one\")\n",
    "\n",
    "The data for each image is made of a matrix of 28 x 28 \n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "so as shown above we have an X and Y rows and columns with data of 0 to 255 in it.\n",
    "Now to achieve such data we must ensure that the image is in binary color (Black and White)\n",
    "this can be achieved using the `cmap= plot.cm.binary` property in `imshow` method.\n",
    "\n",
    "**_NOTE:_** `cm.binary` inverses the color so all black pixels are changed to white and vice versa.\n",
    "\n",
    "![gray one](./img/one-gray.png \"gray one\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa756f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x155869cd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.imshow(image_train[3], cmap= plot.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8a6d4",
   "metadata": {},
   "source": [
    "# Read pixel data\n",
    "Now we need to check the value of each pixel in the matrix the closer it is to 255 it will be white and the closer to 0 it will be black\n",
    "By looking at `print` result we can see that some pixels are white (255), some are black (0) and some are gray \n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|124|253|255|63|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|96|244|251|253|62|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|127|251|251|253|62|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|68|236|251|211|31|8|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|60|228|251|251|94|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|155|253|253|189|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|20|253|251|235|66|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|32|205|253|251|126|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|104|251|253|184|15|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|80|240|251|193|23|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|32|253|253|253|159|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|151|251|251|251|39|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|48|221|251|251|172|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|234|251|251|196|12|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|253|251|251|89|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|159|255|253|253|31|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|48|228|253|247|140|8|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|64|251|253|220|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|64|251|253|220|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|24|193|253|220|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3824fa",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "In order to train our data we need to normalize it as we have seen the data now contains a grayscale image so it has only 1 channel Gray instead of 3 channel of Red Green Blue (RGB).\n",
    "So the value for each pixel is between 0 to 255. In order to make our model insensative to colors we need to normalise it.\n",
    "\n",
    "Now that we know our data is ranging between 0 to 255 we can try to scale down the range and normalize it using `keras.utils.normalize` function available in `TensorFlow` library which we have imported as `tf`.\n",
    "the `axis=1` property is ensuring normalisation is happening row by row which in our case row or column it doesn't matters as the Matrix is equel in x and y axis (28 x 28).\n",
    "\n",
    "**_NOTE:_** Normalisation will bring all values between 0 and 1 as it is technically deviding every pixel to 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d77b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = tf.keras.utils.normalize(image_train, axis=1)\n",
    "image_test = tf.keras.utils.normalize(image_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96458dcc",
   "metadata": {},
   "source": [
    "Now we can see that the color scale \n",
    "\n",
    "|Original Data| Gray Scaled (Binary)| Normalised |\n",
    "|-|-|-|\n",
    "|![colored one](./img/one-color.png \"colored one\")|![gray one](./img/one-gray.png \"gray one\")|![gray normalize one](./img/one-gray-norm.png \"gray normalize one\")|\n",
    "\n",
    "**_NOTE:_** As we will be using a deep learning model we need to have labeled data to explain to the model that for example the above images are one and these labeled data are stored in `label_test` and `label_train`\n",
    "\n",
    "# Preparation for CNN\n",
    "We will first resize the images and we need to increase the data diemention by 1 to create a 3 diementional matrix for kernel(Filter) operation. \n",
    "In python `-1` corresponds to MAX Array Size which in our case is 60000 for train samples and 10000 for test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f85ae467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "IMAGE_SIZE=28\n",
    "image_trainer = np.array(image_train).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "image_tester = np.array(image_test).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3325506",
   "metadata": {},
   "source": [
    "# Creating Deep Learning Neural Network\n",
    "We can use Tensorflow library to create sequentially connected deep learning layers such as `Dense`, `Dropout`, `Activation`, `Flatten`, `Conv2D`, `MaxPooling2D`\n",
    "\n",
    "![sequentially connected deep learning layers](./img/deeplearning-layers.jpg \"sequentially connected deep learning layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee55c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "dnnModel = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7c151",
   "metadata": {},
   "source": [
    "## Adding the first Convolutional layer\n",
    "We add a 64 different filter which each filter has a 3 x 3 size.\n",
    "So 64 filters will be convoulved in this image data.\n",
    "\n",
    "![first Convolutional layer](./img/first-con-layer.png \"first Convolutional layer\")\n",
    "\n",
    "Then we apply the activation layer to ensure that values less than zero are dropped and values larger than zero passes to next layer.\n",
    "On next layer we add a Max pooling layer which allows only a max value of a 2 x 2 data is passed and rest is droped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eb0f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Conv2D(64, (3,3), input_shape = image_trainer.shape[1:]))\n",
    "dnnModel.add(Activation(\"relu\"))\n",
    "dnnModel.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ffaae",
   "metadata": {},
   "source": [
    "## Adding Second Convolutional layer\n",
    "By adding a new layer we will check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a201fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Conv2D(64, (3,3), input_shape = image_trainer.shape[1:]))\n",
    "dnnModel.add(Activation(\"relu\"))\n",
    "dnnModel.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d7bac",
   "metadata": {},
   "source": [
    "## Adding Third Convolutional layer\n",
    "If we need more accuracy then a third layer is added\n",
    "This is done by Trial and error and comparision of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a6c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Conv2D(64, (3,3), input_shape = image_trainer.shape[1:]))\n",
    "dnnModel.add(Activation(\"relu\"))\n",
    "dnnModel.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f39aa0",
   "metadata": {},
   "source": [
    "## Adding First Fully Connected Layer\n",
    "At this stage we need to flatten the layers and this flatten layer is converted from a 2 dimentional to one dimentional.\n",
    "Similarly a Dense layer is added which is a neural network layer as all neurans are connected.\n",
    "Technically each filter in this architecture are neuron.\n",
    "Finally we add the activation layer.\n",
    "\n",
    "\n",
    "![Fully Connected Layer](./img/fully-connected-layer-cnn.png \"Fully Connected Layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55b72108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Flatten())\n",
    "dnnModel.add(Dense(64))\n",
    "dnnModel.add(Activation(\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dffb306",
   "metadata": {},
   "source": [
    "## Adding Second Fully Connected Layer\n",
    "We need to reduce the size on each stage so by using `Dense(32)` we are redusing the layer size to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd3e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Dense(32))\n",
    "dnnModel.add(Activation(\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71d92e",
   "metadata": {},
   "source": [
    "## Adding Last Fully Connected Layer\n",
    "And our final size is 10 as we want to map each item to a digit from 0 to 9 so we need to dense the layer to 10 at this stage.\n",
    "At this stage we change the activation layer to `softmax` which gives us the class probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e1ae318",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Dense(10))\n",
    "dnnModel.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617da5f",
   "metadata": {},
   "source": [
    "### Verify the architecure\n",
    "We can use `dnnModel.summary()` method on our model to have a glimpse on the sequentional model which we have created.\n",
    "\n",
    "#### Model: \"sequential\"\n",
    " |Layer (type)  |              Output Shape     |         Param #|   \n",
    "|-|-|-|\n",
    "| conv2d (Conv2D)             | (None, 26, 26, 64)   |     640       |                                                               \n",
    "| activation (Activation)     | (None, 26, 26, 64)   |     0         |                                                               \n",
    "| max_pooling2d (MaxPooling2D)  | (None, 13, 13, 64)   |    0        |                                                               \n",
    "| conv2d_1 (Conv2D)           | (None, 11, 11, 64)   |     36928     |                                                               \n",
    "| activation_1 (Activation)   | (None, 11, 11, 64)   |     0         |                                                               \n",
    "| max_pooling2d_1 (MaxPooling2D)  | (None, 5, 5, 64)     |    0      | \n",
    "| conv2d_2 (Conv2D)           | (None, 3, 3, 64)     |     36928     |                                                               \n",
    "| activation_2 (Activation)   | (None, 3, 3, 64)     |     0         |                                                               \n",
    "| max_pooling2d_2 (MaxPooling2D)  | (None, 1, 1, 64)     |    0      |                                                                 \n",
    "| flatten (Flatten)           | (None, 64)           |     0         |                                                               \n",
    "| dense (Dense)               | (None, 64)           |     4160      |                                                               \n",
    "| activation_3 (Activation)   | (None, 64)           |     0         |                                                               \n",
    "| dense_1 (Dense)             | (None, 32)           |     2080      |                                                               \n",
    "| activation_4 (Activation)   | (None, 32)           |     0         |                                                               \n",
    "| dense_2 (Dense)             | (None, 10)           |     330       |                                                               \n",
    "| activation_5 (Activation)   | (None, 10)           |     0         |                                                               \n",
    "\n",
    "- Total params: 81,066\n",
    "- Trainable params: 81,066\n",
    "- Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7d257",
   "metadata": {},
   "source": [
    "# Compiling and Training the Model\n",
    "We compile our model using loss property of `sparse_categorical_crossentropy` with an optimizer of `adam` which is one of the best optimizers.\n",
    "And for metrics we want to focus on `accuracy` \n",
    "\n",
    "We will train the model only for 5 epochs and we can see that we reach to a accuracy of `0.9851` and our validation accuracy is `0.9830` which is very good result and we are not overfitting our model.\n",
    "If incase overfitting would have occured we could have added `Dropout` layers to overcome the issue.\n",
    "\n",
    "| Epoch |  loss | accuracy | val_loss | val_accuracy |\n",
    "|-|-|-|-|-|\n",
    "| 1 | 0.3224|0.8983 | 0.1337|0.9584 |\n",
    "| 2 | 0.1052|0.9682 |0.0900 | 0.9730|\n",
    "| 3 | 0.0748|0.9770 |0.0809 | 0.9761|\n",
    "| 4 | 0.0593| 0.9821| 0.0742| 0.9778|\n",
    "| 5 |0.0471 | 0.9851| 0.0559| 0.9830|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11bd8966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 19:52:23.914383: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 18s 14ms/step - loss: 0.3299 - accuracy: 0.8940 - val_loss: 0.1637 - val_accuracy: 0.9500\n",
      "Epoch 2/5\n",
      "1313/1313 [==============================] - 19s 15ms/step - loss: 0.1019 - accuracy: 0.9690 - val_loss: 0.0905 - val_accuracy: 0.9717\n",
      "Epoch 3/5\n",
      "1313/1313 [==============================] - 23s 18ms/step - loss: 0.0725 - accuracy: 0.9776 - val_loss: 0.0714 - val_accuracy: 0.9777\n",
      "Epoch 4/5\n",
      "1313/1313 [==============================] - 20s 15ms/step - loss: 0.0555 - accuracy: 0.9824 - val_loss: 0.0628 - val_accuracy: 0.9808\n",
      "Epoch 5/5\n",
      "1313/1313 [==============================] - 20s 15ms/step - loss: 0.0437 - accuracy: 0.9862 - val_loss: 0.0851 - val_accuracy: 0.9763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16bffdcd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnnModel.compile(loss= \"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "dnnModel.fit(image_trainer, label_train, epochs=5, validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58a5e9",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Now that we have trained and tested the model we can try and predict to understand if our model is heading to the right direction.\n",
    "As we have used `Activation(\"softmax\")` on our last layer so now we will have many class probability. \n",
    "the maximum of the class probability will be our prediction ansewer.\n",
    "\n",
    "Now that we have a list of prediction we can check few samples to see if the predicted value and the image are matching.\n",
    "for example we have printed prediction number 22 which is showing a value of 6 and as seen in the image we can see that it is `6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b81f9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17ef080d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = dnnModel.predict([image_tester])\n",
    "print (np.argmax(prediction[22]))\n",
    "plot.imshow(image_test[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195576e",
   "metadata": {},
   "source": [
    "# Try with a real handwriting\n",
    "Now that we have a trained model. We can test it with a real handwritten image.\n",
    "\n",
    "We created this image in photoshop and wrote the number using mouse and saved the image in `./img/handwritten-number.jpg`.\n",
    "\n",
    "![Handwritten](./img/handwritten-number.jpg \"Handwritten\")\n",
    "\n",
    "Now we need to convert the image and make sure it fits the diemention of our trained data which was 28 x 28 and it was in grayscaled.\n",
    "This can be achied by using the `cvtColor` method and `resize` method available in `cv2` library.\n",
    "\n",
    "Last but not least we need to remember to normalize our image and modify its size for kernel (filter) operation of convolution layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67907d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "yourImage = cv2.imread('./img/handwritten-number.jpg')\n",
    "yourImage = cv2.cvtColor(yourImage, cv2.COLOR_BGR2GRAY)\n",
    "yourImage = cv2.resize(yourImage, (28,28), interpolation=cv2.INTER_AREA)\n",
    "yourImage = tf.keras.utils.normalize(yourImage, axis= 1)\n",
    "yourImage = np.array(yourImage).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a05f0",
   "metadata": {},
   "source": [
    "## Predict your handwritten image\n",
    "Now that the image is ready and proccessed we can use our trained model to predict the handwritten digit using `dnnModel.predict()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "008709f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "prediction = dnnModel.predict(yourImage)\n",
    "print(np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a15c2",
   "metadata": {},
   "source": [
    "We can see that the predicted value is the same as our handwritten image.\n",
    "To avoid running the entire training section we can also save the trained model using the `save()` command and load it whenever needed using `tf.keras.models.load_model()` method in Tensor flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd66465",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.save('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12e9e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel = tf.keras.models.load_model('./model/')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
