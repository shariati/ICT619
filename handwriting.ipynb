{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3859d",
   "metadata": {},
   "source": [
    "# Handwritten DIGITalization \n",
    "\n",
    "We used TensorFlow, an open-source software library and ‘MNIST’ data set adapted from\n",
    "Homepage: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "| Split | Example |\n",
    "|-|-|\n",
    "|test|10,000|\n",
    "|train|60,000|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0908ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "handDigitalizationDataset = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165616e7",
   "metadata": {},
   "source": [
    "The `handDigitalizationDataset` contains data of handwritten characters based on a 28x28 pixels images of 0 to 9 as shown below\n",
    "![28x28 pixels images of 0 to 9](./img/mnist-3.0.1.png \"28x28 pixels images of 0 to 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b8006",
   "metadata": {},
   "source": [
    "# Spliting the Data\n",
    "`handDigitalizationDataset` will be devided to two groups of `test` and `train` data.\n",
    "\n",
    "As we are dealing with a two diementional data (rows and columns) so we have to create two sets of test and train data for each diemention.\n",
    "We know that each row contains image data and each column contains the label (value).\n",
    "for train data we have created `image_train` and `label_train` and for test data `image_test` and `label_test`.\n",
    "\n",
    "**_NOTE:_** Testing should never be part of training data since it will assist us to validate the performance of our neural network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7357f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(image_train, label_train), (image_test, label_test) = handDigitalizationDataset.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c93a7",
   "metadata": {},
   "source": [
    "To verify the data dimension, we may use the `shape` command, which is a tuple that always provides the array's dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92020e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad84e9e",
   "metadata": {},
   "source": [
    "# Loading Image Library\n",
    "To plot the images in the dataset we rely on `matplotlib.pyplot` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591e7e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "plot.imshow(image_train[0])\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a6fb5",
   "metadata": {},
   "source": [
    "## The image matrix\n",
    "We can see that the image data in the dataset are consist of colored image \n",
    "\n",
    "![colored one](./img/one-color.png \"colored one\")\n",
    "\n",
    "The data for each image is made of a matrix of 28 x 28 \n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "so as shown above we have an X and Y rows and columns with data of 0 to 255 in it.\n",
    "Now to achieve such data we must ensure that the image is in binary color (Black and White)\n",
    "this can be achieved using the `cmap= plot.cm.binary` property in `imshow` method.\n",
    "\n",
    "**_NOTE:_** `cm.binary` inverses the color so all black pixels are changed to white and vice versa.\n",
    "\n",
    "![gray one](./img/one-gray.png \"gray one\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa756f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x169076100>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.imshow(image_train[0], cmap= plot.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8a6d4",
   "metadata": {},
   "source": [
    "# Read pixel data\n",
    "Now we need to check the value of each pixel in the matrix the closer it is to 255 it will be white and the closer to 0 it will be black\n",
    "By looking at `print` result we can see that some pixels are white (255), some are black (0) and some are gray \n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|124|253|255|63|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|96|244|251|253|62|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|127|251|251|253|62|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|68|236|251|211|31|8|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|60|228|251|251|94|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|155|253|253|189|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|20|253|251|235|66|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|32|205|253|251|126|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|104|251|253|184|15|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|80|240|251|193|23|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|32|253|253|253|159|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|151|251|251|251|39|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|48|221|251|251|172|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|234|251|251|196|12|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|253|251|251|89|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|159|255|253|253|31|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|48|228|253|247|140|8|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|64|251|253|220|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|64|251|253|220|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|24|193|253|220|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3824fa",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "In order to train our data we need to normalize it as we have seen the data now contains a grayscale image so it has only 1 channel Gray instead of 3 channel of Red Green Blue (RGB).\n",
    "So the value for each pixel is between 0 to 255. In order to make our model insensative to colors we need to normalise it.\n",
    "\n",
    "Now that we know our data is ranging between 0 to 255 we can try to scale down the range and normalize it using `keras.utils.normalize` function available in `TensorFlow` library which we have imported as `tf`.\n",
    "the `axis=1` property is ensuring normalisation is happening row by row which in our case row or column it doesn't matters as the Matrix is equel in x and y axis (28 x 28).\n",
    "\n",
    "**_NOTE:_** Normalisation will bring all values between 0 and 1 as it is technically deviding every pixel to 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d77b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = tf.keras.utils.normalize(image_train, axis=1)\n",
    "image_test = tf.keras.utils.normalize(image_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96458dcc",
   "metadata": {},
   "source": [
    "Now we can see that the color scale \n",
    "\n",
    "|Original Data| Gray Scaled (Binary)| Normalised |\n",
    "|-|-|-|\n",
    "|![colored one](./img/one-color.png \"colored one\")|![gray one](./img/one-gray.png \"gray one\")|![gray normalize one](./img/one-gray-norm.png \"gray normalize one\")|\n",
    "\n",
    "**_NOTE:_** As we will be using a deep learning model we need to have labeled data to explain to the model that for example the above images are one and these labeled data are stored in `label_test` and `label_train`\n",
    "\n",
    "# Preparation for CNN\n",
    "We will first resize the images and we need to increase the data diemention by 1 to create a 3 diementional matrix for kernel(Filter) operation. \n",
    "In python `-1` corresponds to MAX Array Size which in our case is 60000 for train samples and 10000 for test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f85ae467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "IMAGE_SIZE=28\n",
    "image_trainer = np.array(image_train).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "image_tester = np.array(image_test).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3325506",
   "metadata": {},
   "source": [
    "# Creating Deep Learning Neural Network\n",
    "We can use Tensorflow library to create sequentially connected deep learning layers such as `Dense`, `Dropout`, `Activation`, `Flatten`, `Conv2D`, `MaxPooling2D`\n",
    "\n",
    "![sequentially connected deep learning layers](./img/deeplearning-layers.jpg \"sequentially connected deep learning layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee55c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "dnnModel = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7c151",
   "metadata": {},
   "source": [
    "## Adding the first Convolutional layer\n",
    "We add a 64 different filter which each filter has a 3 x 3 size.\n",
    "So 64 filters will be convoulved in this image data.\n",
    "\n",
    "![first Convolutional layer](./img/first-con-layer.png \"first Convolutional layer\")\n",
    "\n",
    "Then we apply the activation layer to ensure that values less than zero are dropped and values larger than zero passes to next layer.\n",
    "On next layer we add a Max pooling layer which allows only a max value of a 2 x 2 data is passed and rest is droped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eb0f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Conv2D(64, (3,3), input_shape = image_trainer.shape[1:]))\n",
    "dnnModel.add(Activation(\"relu\"))\n",
    "dnnModel.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ffaae",
   "metadata": {},
   "source": [
    "## Adding Second Convolutional layer\n",
    "By adding a new layer we will check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a201fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Conv2D(64, (3,3), input_shape = image_trainer.shape[1:]))\n",
    "dnnModel.add(Activation(\"relu\"))\n",
    "dnnModel.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d7bac",
   "metadata": {},
   "source": [
    "## Adding Third Convolutional layer\n",
    "If we need more accuracy then a third layer is added\n",
    "This is done by Trial and error and comparision of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a6c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Conv2D(64, (3,3), input_shape = image_trainer.shape[1:]))\n",
    "dnnModel.add(Activation(\"relu\"))\n",
    "dnnModel.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f39aa0",
   "metadata": {},
   "source": [
    "## Adding First Fully Connected Layer\n",
    "At this stage we need to flatten the layers and this flatten layer is converted from a 2 dimentional to one dimentional.\n",
    "Similarly a Dense layer is added which is a neural network layer as all neurans are connected.\n",
    "Technically each filter in this architecture are neuron.\n",
    "Finally we add the activation layer.\n",
    "\n",
    "\n",
    "![Fully Connected Layer](./img/fully-connected-layer-cnn.png \"Fully Connected Layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55b72108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Flatten())\n",
    "dnnModel.add(Dense(64))\n",
    "dnnModel.add(Activation(\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dffb306",
   "metadata": {},
   "source": [
    "## Adding Second Fully Connected Layer\n",
    "We need to reduce the size on each stage so by using `Dense(32)` we are redusing the layer size to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd3e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Dense(32))\n",
    "dnnModel.add(Activation(\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71d92e",
   "metadata": {},
   "source": [
    "## Adding Last Fully Connected Layer\n",
    "And our final size is 10 as we want to map each item to a digit from 0 to 9 so we need to dense the layer to 10 at this stage.\n",
    "At this stage we change the activation layer to `softmax` which gives us the class probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e1ae318",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.add(Dense(10))\n",
    "dnnModel.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617da5f",
   "metadata": {},
   "source": [
    "### Verify the architecure\n",
    "We can use `dnnModel.summary()` method on our model to have a glimpse on the sequentional model which we have created.\n",
    "\n",
    "#### Model: \"sequential\"\n",
    " |Layer (type)  |              Output Shape     |         Param #|   \n",
    "|-|-|-|\n",
    "| conv2d (Conv2D)             | (None, 26, 26, 64)   |     640       |                                                               \n",
    "| activation (Activation)     | (None, 26, 26, 64)   |     0         |                                                               \n",
    "| max_pooling2d (MaxPooling2D)  | (None, 13, 13, 64)   |    0        |                                                               \n",
    "| conv2d_1 (Conv2D)           | (None, 11, 11, 64)   |     36928     |                                                               \n",
    "| activation_1 (Activation)   | (None, 11, 11, 64)   |     0         |                                                               \n",
    "| max_pooling2d_1 (MaxPooling2D)  | (None, 5, 5, 64)     |    0      | \n",
    "| conv2d_2 (Conv2D)           | (None, 3, 3, 64)     |     36928     |                                                               \n",
    "| activation_2 (Activation)   | (None, 3, 3, 64)     |     0         |                                                               \n",
    "| max_pooling2d_2 (MaxPooling2D)  | (None, 1, 1, 64)     |    0      |                                                                 \n",
    "| flatten (Flatten)           | (None, 64)           |     0         |                                                               \n",
    "| dense (Dense)               | (None, 64)           |     4160      |                                                               \n",
    "| activation_3 (Activation)   | (None, 64)           |     0         |                                                               \n",
    "| dense_1 (Dense)             | (None, 32)           |     2080      |                                                               \n",
    "| activation_4 (Activation)   | (None, 32)           |     0         |                                                               \n",
    "| dense_2 (Dense)             | (None, 10)           |     330       |                                                               \n",
    "| activation_5 (Activation)   | (None, 10)           |     0         |                                                               \n",
    "\n",
    "- Total params: 81,066\n",
    "- Trainable params: 81,066\n",
    "- Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7d257",
   "metadata": {},
   "source": [
    "# Compiling and Training the Model\n",
    "We compile our model using loss property of `sparse_categorical_crossentropy` with an optimizer of `adam` which is one of the best optimizers.\n",
    "And for metrics we want to focus on `accuracy` \n",
    "\n",
    "We will train the model only for 5 epochs and we can see that we reach to a accuracy of `0.9851` and our validation accuracy is `0.9830` which is very good result and we are not overfitting our model.\n",
    "If incase overfitting would have occured we could have added `Dropout` layers to overcome the issue.\n",
    "\n",
    "| Epoch |  loss | accuracy | val_loss | val_accuracy |\n",
    "|-|-|-|-|-|\n",
    "| 1 | 0.3224|0.8983 | 0.1337|0.9584 |\n",
    "| 2 | 0.1052|0.9682 |0.0900 | 0.9730|\n",
    "| 3 | 0.0748|0.9770 |0.0809 | 0.9761|\n",
    "| 4 | 0.0593| 0.9821| 0.0742| 0.9778|\n",
    "| 5 |0.0471 | 0.9851| 0.0559| 0.9830|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd8966",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnModel.compile(loss= \"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "dnnModel.fit(image_trainer, label_train, epochs=5, validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58a5e9",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Now that we have trained and tested the model we can try and predict to understand if our model is heading to the right direction.\n",
    "As we have used `Activation(\"softmax\")` on our last layer so now we will have many class probability. \n",
    "the maximum of the class probability will be our prediction ansewer.\n",
    "\n",
    "Now that we have a list of prediction we can check few samples to see if the predicted value and the image are matching.\n",
    "for example we have printed prediction number 22 which is showing a value of 6 and as seen in the image we can see that it is `6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b81f9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16c3655e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = dnnModel.predict([image_tester])\n",
    "print (np.argmax(prediction[22]))\n",
    "plot.imshow(image_test[22])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
